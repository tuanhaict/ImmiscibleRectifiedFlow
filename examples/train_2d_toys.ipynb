{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/lqiang67/rectified-flow/blob/main/examples/train_2d_toys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lqiang67/rectified-flow.git\n",
    "%cd rectified-flow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectified Flow: 2D Toy Example\n",
    "\n",
    "This notebook provides an example illustrating the basic concept of Rectified Flow and demonstrates training on a 2D toy example. \n",
    "\n",
    "You can check on [this blog post](https://rectifiedflow.github.io/blog/2024/intro/) for a quick introduction.\n",
    "\n",
    "Rectified Flow learns an ordinary differential equation (ODE), $ \\dot{Z}_t = v(Z_t, t) $, to transfer data from a source distribution, $ \\pi_0 $, to a target distribution, $ \\pi_1 $, given limited observed data points sampled from $ \\pi_1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F1exIRI93Jzv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.distributions as dist\n",
    "\n",
    "from rectified_flow.utils import set_seed\n",
    "from rectified_flow.utils import visualize_2d_trajectories_plotly\n",
    "\n",
    "from rectified_flow.rectified_flow import RectifiedFlow\n",
    "\n",
    "set_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Distributions $ \\pi_0 $ and $ \\pi_1 $\n",
    "\n",
    "In this section, we generate synthetic $ \\pi_0 $ and $ \\pi_1 $ as two Gaussian mixture models (GMM).\n",
    "\n",
    "We sample $50,000$ data points from each distribution and store them as `D0` and `D1`. Additionally, we store the labels for $ \\pi_1 $ to differentiate whether the points belong to the upper or lower part of the $\\pi_1$ GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.datasets.toy_gmm import TwoPointGMM\n",
    "\n",
    "n_samples = 50000\n",
    "pi_0 = TwoPointGMM(x=0.0, y=7.5, std=0.5, device=device)\n",
    "pi_1 = TwoPointGMM(x=15.0, y=7.5, std=0.5, device=device)\n",
    "D0 = pi_0.sample([n_samples])\n",
    "D1, labels = pi_1.sample_with_labels([n_samples])\n",
    "labels.tolist()\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.title(r'Samples from $\\pi_0$ and $\\pi_1$')\n",
    "plt.scatter(D0[:, 0].cpu(), D0[:, 1].cpu(), alpha=0.5, label=r'$\\pi_0$')\n",
    "plt.scatter(D1[:, 0].cpu(), D1[:, 1].cpu(), alpha=0.5, label=r'$\\pi_1$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "## 1-Rectified Flow\n",
    "\n",
    "Given observed samples $X_0 \\sim \\pi_0$ from source distribution and $X_1 \\sim \\pi_1$ from target distribution, the *rectified flow* induced by coupling $(X_0, X_1)$ is the time-differentiable process $\\mathbf{Z} = \\{Z_t: t \\in [0, 1]\\}$ with the velocity field defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{d}Z_t = v(Z_t, t) \\, \\mathrm{d}t, \\quad t \\in [0, 1], \\quad \\text{starting from } Z_0 = X_0.\n",
    "$$\n",
    "\n",
    "Here, $v: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d$ is set in a way that ensures that $Z_1$ follows $\\pi_1$ when $Z_0 \\sim \\pi_0$.\n",
    "\n",
    "Denote $X_t = \\alpha_t \\cdot X_1 + \\beta_t \\cdot X_0$ as an interpolation of samples $X_0$ and $X_1$. The velocity field is given by:\n",
    "\n",
    "$$\n",
    "v(z, t) = \\mathbb{E}[ \\dot X_t \\mid X_t = z] = \\arg \\min_{v} \\int_0^1 \\mathbb{E}\\left[\\lVert  \\dot \\alpha_t X_1 + \\dot \\beta_t X_0 - v(X_t, t) \\rVert^2\\right] \\mathrm{d}t,\n",
    "$$\n",
    "Here, $\\alpha_t, \\beta_t$ are any differentiable functions of time $t$ that satisfy $\\alpha_0=\\beta_1=0$ and $\\alpha_1 = \\beta_0 = 1$, $v(z,t)$ is the conditional expectation of all $\\dot X_t$ at $X_t=z$.\n",
    "\n",
    "We call the process $\\{Z_t\\}$ the **rectified flow** induced from the interpolation $\\{X_t\\}$.\n",
    "\n",
    "The default choice is the straight interpolation:\n",
    "$$\n",
    "X_t = t X_1 + (1-t) X_0, \\quad \\quad \\dot X_t = X_1 - X_0.\n",
    "$$\n",
    "\n",
    "Let's first visualize this straight interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = pi_0.sample([500])\n",
    "x_0_upper = x_0.clone()\n",
    "x_0_upper[:, 1] = torch.abs(x_0_upper[:, 1])\n",
    "x_0_lower = x_0.clone()\n",
    "x_0_lower[:, 1] = -torch.abs(x_0_lower[:, 1])\n",
    "\n",
    "x_1_upper = pi_1.sample([500])\n",
    "x_1_lower = pi_1.sample([500])\n",
    "\n",
    "interp_upper = []\n",
    "interp_lower = []\n",
    "\n",
    "for t in np.linspace(0, 1, 100):\n",
    "    x_t_uppper = (1 - t) * x_0_upper + t * x_1_upper\n",
    "    x_t_lower = (1 - t) * x_0_lower + t * x_1_lower\n",
    "    interp_upper.append(x_t_uppper)\n",
    "    interp_lower.append(x_t_lower)\n",
    "    \n",
    "visualize_2d_trajectories_plotly(\n",
    "    trajectories_dict={\n",
    "        \"upper\": interp_upper,\n",
    "\t\t\"lower\": interp_lower\n",
    "    },\n",
    "    D1_gt_samples=torch.cat([x_1_upper, x_1_lower], dim=0),\n",
    "    num_trajectories=100,\n",
    "\ttitle=\"Straight Interpolation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This straight interpolation successfully constructs paths to transport $\\pi_0$ to $\\pi_1$. However, we cannot \"simulate\" these paths from $X_0$ because:\n",
    "\n",
    "- The updates at each position $X_t$ depend on the final state $X_1$, which is inaccessible at intermediate times ($t < 1$).\n",
    "\n",
    "In the figure above, trajectories intersect at the middle (drag `step` to $50$), indicating that there is **multiple possible diretions** to $\\pi_1$. \n",
    "\n",
    "Such behavior makes it impossible to simulate using ODEs, as ODEs require a unique direction (or velocity) $v_t(X_t)$ for the given current state $(X_t, t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a Rectified Flow Velocity with MLP\n",
    "\n",
    "We parameterize the velocity field using a small unconditional MLP $v_\\theta$.\n",
    "\n",
    "The model is then passed to the `RectifiedFlow` class. In this 2D toy example, the data shape is `(2,)`, and we use the `\"straight\"` interpolation mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.models.toy_mlp import MLPVelocity\n",
    "\n",
    "model = MLPVelocity(2, hidden_sizes = [128, 128, 128]).to(device)\n",
    "\n",
    "rectified_flow = RectifiedFlow(\n",
    "    data_shape=(2,),\n",
    "    velocity_field=model,\n",
    "    interp=\"straight\",\n",
    "    source_distribution=pi_0,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the model samples data points from the source ($\\pi_0$) and target ($\\pi_1$) distributions to compute the loss for optimizing the velocity field:\n",
    "\n",
    "$$\n",
    "\\ell = \\min_{\\theta} \n",
    "\\int_0^1 \\mathbb{E}_{X_0 \\sim \\pi_0, X_1 \\sim \\pi_1} \\left [ \\left\\| (X_1 - X_0) - v_\\theta(X_t, t) \\right\\|^2 \\right ] \\mathrm{d}t, \n",
    "\\quad \\text{where} \\quad\n",
    "X_t = t X_1 + (1-t) X_0.\n",
    "$$\n",
    "\n",
    "The `get_loss` method computes the rectified flow loss using:\n",
    "- **Inputs**:\n",
    "  - `x_0`: Samples from $\\pi_0$.\n",
    "  - `x_1`: Samples from $\\pi_1$.\n",
    "  - `labels` (optional): Provides conditional information, e.g., GMM component idx.\n",
    "- **Steps**:\n",
    "  1. **Interpolation**: Computes intermediate states $X_t$ and derivatives $\\dot{X}_t$.\n",
    "  2. **Prediction**: Predicts $v_\\theta(X_t, t)$ using the velocity model.\n",
    "  3. **Loss**: Measures the loss between $v_\\theta(X_t, t)$ and $\\dot{X}_t$, with time-dependent weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "qqQwOYJFj5dw",
    "outputId": "b2f12485-d699-417a-b575-683f58fa95ad"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 1024\n",
    "\n",
    "losses = []\n",
    "\n",
    "for step in range(5000):\n",
    "\toptimizer.zero_grad()\n",
    "\tidx = torch.randperm(n_samples)[:batch_size]\n",
    "\tx_0 = D0[idx].to(device)\n",
    "\tx_1 = D1[idx].to(device)\n",
    "\t\n",
    "\tloss = rectified_flow.get_loss(x_0, x_1)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tlosses.append(loss.item())\n",
    "\n",
    "\tif step % 1000 == 0:\n",
    "\t\tprint(f\"Epoch {step}, Loss: {loss.item()}\")\n",
    "    \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the rectified flows trained using straight interpolation, we can now visualize the trajectories to observe how the rectified flow effectively **'causalizes'** the interpolation process.\n",
    "\n",
    "We split $X_0 \\sim \\pi_0$ into two categories: points above and below the $X$-axis. We then used these subsets of $X_0$ to perform sampling with the rectified flows.\n",
    "\n",
    "From the visualization, we can observe that the trajectories are \"rewired\" at the middle - there are no intersections between trajectories, and the blue and pink dots evolve separately, meaning that they are now \"simulatable\".\n",
    "\n",
    "This reflects how rectified flow learns the average direction at points of intersection with:\n",
    "\n",
    "$$\n",
    "v(z, t) = \\mathbb{E}[\\dot{X}_t \\mid X_t = z].\n",
    "$$\n",
    "\n",
    "**Intuition of \"average\"**\n",
    "\n",
    "A critical intuition here is that this average does not change the total amount of mass or the number of \"particles\" passing through the intersection, thereby preserving the same distribution as $\\{X_t\\}$ at every time $t$. \n",
    "\n",
    "Check the trajectories below: when $t$ is around $0.5$, the number of particles moving to the right side has not changed; they have merely swapped trajectories. The number of particles on the interpolation path is approximately the same as the number on the learned rectified flow path.\n",
    "\n",
    "![cross](../assets/flow_in_out.png)\n",
    "\n",
    "Note: Due to the error introduced by Euler discretization, some particles may have moved to the other side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "2df4fiUtldJf",
    "outputId": "df915f27-8cd9-457c-bf30-a297979072e9"
   },
   "outputs": [],
   "source": [
    "from rectified_flow.samplers import EulerSampler\n",
    "from rectified_flow.utils import visualize_2d_trajectories_plotly\n",
    "\n",
    "euler_sampler_1rf_unconditional = EulerSampler(\n",
    "    rectified_flow=rectified_flow,\n",
    "    num_steps=100,\n",
    ")\n",
    "\n",
    "traj_upper = euler_sampler_1rf_unconditional.sample_loop(x_0=x_0_upper).trajectories\n",
    "traj_lower = euler_sampler_1rf_unconditional.sample_loop(x_0=x_0_lower).trajectories\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "    trajectories_dict={\"upper\": traj_upper, \"lower\": traj_lower},\n",
    "    D1_gt_samples=D1[:1000],\n",
    "    num_trajectories=200,\n",
    "\ttitle=\"Unconditional 1-Rectified Flow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectories of $\\{Z_t\\}$ are not straight; therefore, the one-step result does not yield accurate results.\n",
    "\n",
    "$$\n",
    "\\hat{X}_1 = X_0 + v(X_0, 0).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_sampler_1rf_unconditional.sample_loop(num_steps=1, seed=0)\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "    {\"1rf uncond one-step\": euler_sampler_1rf_unconditional.trajectories}, \n",
    "    D1[:1000],\n",
    "    num_trajectories=200,\n",
    "\ttitle=\"Unconditional 1-Rectified Flow, 1-step\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a Conditional Rectified Flow\n",
    "\n",
    "The rectified flow model can be extended to include class conditioning. By passing class information $c \\in \\{0, 1\\}$ (e.g., GMM components index), the velocity field becomes class-dependent.\n",
    "\n",
    "$$\n",
    "\\ell = \\min_{\\theta}\n",
    "\\int_0^1 \\mathbb{E}_{X_0 \\sim \\pi_0, (X_1,c) \\sim \\pi_1} \\left [ \\left\\| (X_1 - X_0) - v_\\theta(X_t, t, c) \\right\\|^2 \\right ] \\mathrm{d}t,\n",
    "\\quad \\text{where} \\quad\n",
    "X_t = t X_1 + (1-t) X_0.\n",
    "$$\n",
    "\n",
    "In this case, $(X_1, c)\\sim \\pi_1$ is the distribution of data-label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.models.toy_mlp import MLPVelocityConditioned\n",
    "\n",
    "model_cond = MLPVelocityConditioned(2, hidden_sizes = [128, 128, 128]).to(device)\n",
    "\n",
    "rectified_flow_cond = RectifiedFlow(\n",
    "    data_shape=(2,),\n",
    "    velocity_field=model_cond,\n",
    "    interp=\"straight\",\n",
    "    source_distribution=pi_0,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_cond.parameters(), lr=1e-3)\n",
    "batch_size = 1024\n",
    "\n",
    "losses = []\n",
    "\n",
    "for step in range(5000):\n",
    "\toptimizer.zero_grad()\n",
    "\tidx = torch.randperm(n_samples)[:batch_size]\n",
    "\tx_0 = D0[idx]\n",
    "\tx_1, cond = D1[idx], labels[idx]\n",
    "\t\n",
    "\tx_0 = x_0.to(device)\n",
    "\tx_1 = x_1.to(device)\n",
    "\tcond = torch.tensor(cond).to(device)\n",
    "\t\n",
    "\tloss = rectified_flow_cond.get_loss(x_0, x_1, labels=cond)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tlosses.append(loss.item())\n",
    "\n",
    "\tif step % 1000 == 0:\n",
    "\t\tprint(f\"Epoch {step}, Loss: {loss.item()}\")\n",
    "    \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By incorporating class information, the model can better capture the structure of conditional distributions. \n",
    "\n",
    "For instance, given a specific class (e.g., sampling only the upper part of the right-side distribution), the trajectories do not intersect. \n",
    "\n",
    "As a result, the learned velocity is very straight, and even a one-step result performs remarkably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectified_flow.samplers import EulerSampler\n",
    "from rectified_flow.utils import visualize_2d_trajectories_plotly\n",
    "\n",
    "euler_sampler_1rf_conditional = EulerSampler(\n",
    "    rectified_flow=rectified_flow_cond,\n",
    "    num_steps=1,\n",
    "    num_samples=500,\n",
    ")\n",
    "\n",
    "cond = torch.zeros((500,), device=device)\n",
    "euler_sampler_1rf_conditional.sample_loop(seed=0, labels=cond)\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "    {\"1rf cond\": euler_sampler_1rf_conditional.trajectories},\n",
    "    D1[:1000],\n",
    "    num_trajectories=200,\n",
    "    title=\"Conditional 1-Rectified Flow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflow for 2-Rectified Flow\n",
    "\n",
    "Now let's try the *reflow* procedure to get a straightened rectified flow, \n",
    "denoted as 2-Rectified Flow, by repeating the same procedure on with $(X_0,X_1)$ replaced by  $(Z_0^1, Z_1^1)$, where $(Z_0^1, Z_1^1)$ is the coupling simulated from 1-Rectified Flow.  \n",
    "\n",
    "We sample $50,000$ $Z_0^1$ and generate their corresponding $Z_1^1$ by simulating 1-Rectified Flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_0 = rectified_flow.sample_source_distribution(batch_size=50000)\n",
    "\n",
    "Z_1 = euler_sampler_1rf_unconditional.sample_loop(x_0=Z_0, num_steps=1000).trajectories[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 1024\n",
    "\n",
    "losses = []\n",
    "\n",
    "for step in range(5000):\n",
    "\toptimizer.zero_grad()\n",
    "\tidx = torch.randperm(n_samples)[:batch_size]\n",
    "\tx_0 = Z_0[idx]\n",
    "\tx_1 = Z_1[idx]\n",
    "\t\n",
    "\tx_0 = x_0.to(device)\n",
    "\tx_1 = x_1.to(device)\n",
    "\t\n",
    "\tloss = rectified_flow.get_loss(x_0, x_1)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tlosses.append(loss.item())\n",
    "\n",
    "\tif step % 1000 == 0:\n",
    "\t\tprint(f\"Epoch {step}, Loss: {loss.item()}\")\n",
    "    \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_sampler_2rf = EulerSampler(\n",
    "    rectified_flow=rectified_flow,\n",
    "    num_samples=1000,\n",
    ")\n",
    "\n",
    "euler_sampler_2rf.sample_loop(num_steps=100, seed=0)\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "    {\"2rf\": euler_sampler_2rf.trajectories}, \n",
    "    D1[:1000],\n",
    "    num_trajectories=200,\n",
    "    title=\"Reflow Trajectories, 100-step\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_sampler_2rf.sample_loop(num_steps=1, seed=0)\n",
    "\n",
    "visualize_2d_trajectories_plotly(\n",
    "    {\"2rf one-step\" :euler_sampler_2rf.trajectories}, \n",
    "    D1[:1000],\n",
    "    num_trajectories=200,\n",
    "    title=\"Reflow Trajectories, 1-step\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMWW2rwbL3siZ4bCQz0wyDE",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
